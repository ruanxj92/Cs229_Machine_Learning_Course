写在开篇的话：  
这一系列的笔记是斯坦福大学cs229 machine learning 机器学习的课程笔记。初步的规划是将[课程网站](http://cs229.stanford.edu/syllabus.html)上的资料翻译成中文，同时也作为学习的一个记录。同时由于水平有限，可能有一些不能很好翻译的地方会用自己的话表达。同时课程的视频资源详见[bilibili](https://www.bilibili.com/video/av9912938/)。一共有20节课，一般平均每节课大约4-8小节不等。
本篇原文见[cs229-notes1.pdf](http://cs229.stanford.edu/notes/cs229-notes1.pdf)。
[TOC]

#CS229 课程笔记
吴恩达
##监督学习
让我们用几个简单的例子来开始讨论监督学习问题。假设我们有一个数据集包括了俄勒冈的波特兰市47所住房居住面积和价格：
居住面积(平方英尺)|价格(千美元)
---------------|----------
2104|400
1600|330
2400|369
1416|232
3000|540




这些数据画图是这样的：
![波特兰房价面积图](https://img-blog.csdn.net/20180413120426996?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbW9ucnVhbjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)










给出这样的数据，我们能怎样用一个居住面积的函数来预测住房价格呢？
为了作一些标记供今后使用，我们用$x^{(i)}$ 表示“输入”变量(在这个例子中是居住面积)，也叫做输入**特征**， 同时 $y^{(i)}$ 表示要预测的“输出”或**,目标变量**。一对（$x^{(i)},y^{(i)}$）叫做一个**训练样本**，并且把要用来学习的数据集-一列m个训练样本{$x^{(i)},y^{(i)}$:i=1,...,m}叫做**训练集**。注意记号的上角标“(i)”只是用来表示训练集中的序号，没什么需要解释的。我们也用 $ \cal X $ 表示输入变量的空间，用 $ \cal Y $ 表示输出变量的空间。在该例子中 $ \cal X=Y=\Bbb R $.
为了稍微正式一点来表示监督学习问题，我们的目标是对于给定的训练集，能学习到一个函数*h*(x):$ \cal X \mapsto Y$,使得*h*(x)是一个能很好预测对应y值的函数。由于历史原因，这个函数*h* 称为**假设**。 如图，因此过程像这样：
![监督学习函数](https://img-blog.csdn.net/20180413155118838?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbW9ucnVhbjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)















当需要预测的目标变量连续时，比如说我们房价的例子，我们把这样的学习问题叫做**回归**问题。当y只能取若干较小数量的离散值时（比如说给定居住面积，我们想预测某住所是独栋房还是公寓）我们称之为**分类**问题。
###部分 一
##线性回归
为了使得房价预测的例子更加有趣，让我们考虑一个更加丰富的数据集，其中包括了每栋房子的卧室数量：
居住面积(平方英尺)|卧室数|价格(千美元)
---------------|------|---------
2104|3|400
1600|3|330
2400|3|369
1416|2|232
3000|4|540
...|...|...





其中，x是 $\mathbb R^{2}$ 的二维向量。例如，$x_{1}^{(i)}$ 是训练集中第i所房子的居住面积，$ x_{2}^{(i)} $ 是它的卧室数量。（一般来说，当设计一个学习问题，它会取决于要选取那些特征，所以如果你在收集波特兰的房屋数据，你也许要考虑包含一些其他特征比如说房子里有没有壁炉，卫生间的数量等等。再之后的章节我们会对特征选取讨论更多，但现在暂时只考虑给定的特征。）
为了实行监督学习，我们必须要决定如何在计算机中表示函数或者叫假设。如一开始所决定的，我们选用近似$y作为x$的线性函数：  
$$
h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2} 
$$
上式中、$ \theta_{i} $ 是**参数**（也叫做**权重**）用来将$ \cal X 映射到 Y $ 的线性函数的空间参数化。当不引起歧义时，我们把 $ h_{\theta}(x) 的\theta$ 放到下标，更进一步简化成 $h(x)$。 为了简化，引入 $x_{0}=1$（这是**截距项**）故

$$ 
h(x)= \sum_{i=0}^{n}\theta_{i}x_{i}=\theta^{T}x
$$

上式右侧我们把$\theta, x$ 都看作向量，这里 $n$ 是输入变量的数目（不计$x_{0}$）。
现在，给定一个训练集，我们如何挑选或学习参数$\theta$？一个合理的方法看上去是让$h(x)靠近y$，至少在训练集上要接近。为了正式表达，我们定义一个函数来测量对于每一个$\theta$ 对应的$h(x^{(i)})$ 有多接近对应的$y$。定义**代价函数**（成本函数）：
$$
J(\theta)=\frac{1}{2}\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^{2}
$$
如果你已经见过线形回归，你会注意到这和普通的最小二乘法的代价函数很相似。无论之前是否见过，我们最终会知道这是更广泛算法中的一个特例。
##1 LMS算法
我们想通过选择$\theta$ 来使 $J(\theta)$最小化。为了找到这样的$\theta$, 我们用一些“猜测”的初值来开始搜索算法，然后不断改变$\theta$使得$J(\theta)$ 更小，直到收敛到一个$\theta$ 的值使得$J(\theta)$ 最小化。特别的，考虑梯度下降算法，该算法从一些初值$\theta$开始，重复以下操作更新 [^footnote]：
$$
\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta).
$$
(该更新同时对所有$j=0,...,n.$进行操作)式中$\alpha$ 称作**学习率**。该算法自然地重复向$J$ 下降最陡峭的方向前进一步。
为了实现该算法，我们要推导出右边的偏导项。先从只有一个训练样本（$x,y$）开始,忽略$J$ 定义的求和项。我们有：
$$
\begin{align}
\frac{\partial}{\partial\theta_{j}}J(\theta)  
=&\frac{\partial}{\partial\theta_{j}}\frac{1}{2}(h_\theta(x)-y)^2\\
=&2\cdot\frac12(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_{j}}(h_\theta(x)-y)\\
=&(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_{j}}(\sum_{i=0}^{n}\theta_{i}x_{i}-y)\\
=&(h_\theta(x)-y)x_{j}
\end{align}
$$
对于单个训练样本给出更新法则：
$$
\theta_{j}:=\theta_{j}-\alpha(y^i-h_\theta(x^{(i)}))x^{(i)}_j
$$
这个规则称为**LMS**法则（LMS代表最小均方），也叫做Widrow-Hoff学习规则。该规则有许多看上去自然又符合直觉的特性。比如更新的大小与**误差**项($y^{(i)}-h_{\theta}( {x^{(i)}}) $)成比例；因此，比如遇到一个训练样本预测值很接近$y^{(i)}$的真实值，然后发现不太需要调整参数；相反的，一个对于有大误差的预测值$h_{\theta}{x^{(i)}}$，参数需要有大的调整（比如距离$y^{(i)}$非常远）。

我们已经推导出一个样本的LMS法则。有两种方法来修改这个法则，让它能适用多个样本的训练集。第一个方法是改成如下的算法：
$$
\begin{align}
&重复直到收敛\{\\
&\theta_{j}:=\theta_{j}+\alpha\sum^m_{i=1}(h_\theta(x^{(i)})-y^i)x^{(i)}_j(对于每个j)\\
&\}
\end{align}
$$
读者可以很简单地验证以上更新规则的求和就是$\frac{\partial J}{\partial\theta_{j}}$(J是原始定义)。所以这就是简单对原先代价函数$J$的梯度下降。这个方法看起来在整个训练集的每一个样本的每一步中，称之为**批梯度下降**。注意，一般来说，梯度下降法受到局部极小值的影响。我们提出来是因为线性回归只有一个全局最优点，没有其他局部最优点。因此梯度下降法总是收敛（假设学习率$\alpha$不过大）到最小值。事实上，$J$是凸二次函数。这个是最小化二次函数的梯度下降的例子。
[^footnote]:我们用“$a := b$” 来表示（计算机程序的）赋值操作，设定变量a的数值等于变量b的数值。换句话说，该操作用b的值覆盖a的值。相对的，我们用 “$a = b$"断言事实的状态，a的值等于b的值。\


![二次函数等高线](https://img-blog.csdn.net/20180413203557724?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbW9ucnVhbjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)





上图的椭圆展示了二次函数的等高线。也展示了从（48，30）开始的梯度下降的轨迹。图中的$x$(和直线相接)标记出了梯度下降经过的若干连续的$\theta$值.
当我们在之前的数据集上跑批梯度下降来拟合$\theta$时，我们得到$\theta_{0}=71.27,\theta_{1}=0.1345$,如果我们画出$h_{\theta}(x)$作为$x$（面积）的函数图像，以及训练数据点时，我们得到以下图像：
![数据拟合](https://img-blog.csdn.net/20180413210046629?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbW9ucnVhbjky/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



















如果卧室数量也包括在一个输入特征的话，我们得到$\theta_0=89.60,\theta_1=0.1392,\theta_2=-8.738$。
以上结果由批梯度下降得到。批梯度下降以外，另一个算法效果也非常好。考虑如下算法：
$$
\begin{align}
Loop\{\\
	&for\ i=1 to m,\{\\
		    &\theta_j:=\theta_j+\alpha(y^{(i)-h_\theta(x^{(i)})})x_j^{(i)} (对于每个j)\\
	&\}\\
\}\\&
\end{align}
$$
在这个算法中，我们重复遍历训练集，每次遇到一个训练样本，我们根据单个训练样本的误差的梯度更新参数。这个算法称为**随机梯度下降**(也称**增量梯度下降**)。 当批梯度下降法执行一步更新操作要遍历整个训练集-当m很大时，这个操作很费时-随机梯度下降可以直接开始，然后每过一个训练赝本都可以前进一步。经常是随机梯度下降靠近最小值比批梯度下降快得多。（注意有时候也许永远不能收敛到最小值，参数$\theta$会在$J(\theta)$的最小值附近振荡；但是实际上大多数足够靠近最小值的近似都是真正最小值的一个不错的近似值[^footnote2]）。考虑到以上原因，实际上当训练集足够大，随机梯度下降经常都是比批梯度下降更受欢迎的。

##2 正规方程
梯度下降给出了一种最小化$J$ 的方法。
让我们讨论第二种方法，这次明确地做最小化而不用迭代算法。在这个方法中，我们明确的求对$\theta_j$的导数来并令之为零来I求$J$的最小值。为了不用写大量的代数和整页整页的矩阵的导数，让我们引入一些记号来做矩阵的计算。
[^footnote2]: 正如我们之前所描述的那样，通常随机梯度下降用固定的学习率$\alpha$，但缓慢下降学习率$\alpha$到0，可以让参数收敛到全局最小而不是仅仅在最小值附近震荡。
